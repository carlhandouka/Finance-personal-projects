{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bc2d2f",
   "metadata": {},
   "source": [
    "# Portfolio Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc284eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Import all remaining packages\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cbb81",
   "metadata": {},
   "source": [
    "# Batches selection (not important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f95ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size, seq_length, deltat_in):\n",
    "    n = dataset.shape[0]\n",
    "    time_steps = dataset.shape[1]\n",
    "    idx = np.random.choice(time_steps - seq_length, batch_size)\n",
    "\n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = [dataset[i:i + seq_length] for i in idx]\n",
    "\n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = [dataset[i + deltat_in:i + seq_length + deltat_in] for i in idx]\n",
    "\n",
    "    # Convert the input and output batches to tensors\n",
    "    x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
    "    y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec89de",
   "metadata": {},
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8633eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        # Define and initialize parameters: a weight matrix W and bias b\n",
    "        # Note that the parameter initialize is random!\n",
    "        self.W = torch.nn.Parameter(torch.randn(num_inputs, num_outputs))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(num_outputs))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Implement the forward pass of the dense layer\n",
    "        return x @ self.W + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c8a17",
   "metadata": {},
   "source": [
    "# Lag-transformation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LagTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_weighting(theta, t):\n",
    "        alpha= theta[0] * t**(-theta[1])\n",
    "        return alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def saturating_exponential_threshold(theta, t):\n",
    "        beta= theta[2] - theta[3]*torch.exp(-theta[4]*t)\n",
    "        return beta\n",
    "\n",
    "\n",
    "    def forward(self, theta: torch.Tensor, t: torch.Tensor, input : torch.Tensor):\n",
    "        alpha = self.hyperbolic_weighting(theta, t)\n",
    "        beta  = self.saturating_exponential_threshold(theta, t)\n",
    "        return alpha/beta * torch.tanh(beta * input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1529, 0.1206])\n"
     ]
    }
   ],
   "source": [
    "model=LagTransform()\n",
    "y= model(torch.tensor([0.5,0.5,1.,0.5,0.5]), torch.tensor(10.0), torch.tensor([2.0,1.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61944e97",
   "metadata": {},
   "source": [
    "# Correlation Cleaning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationCleaning:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def biGRU(self, input_size, hidden_size, num_layers, dropout):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bigru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        return self.bigru"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
